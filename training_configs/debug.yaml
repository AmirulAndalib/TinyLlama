model_name: "llama_85M"
run_name: "llama_85M_debug"
project_name: "TinyLlama"


# Training Config
per_node_batch_size: 128
learning_rate: 0.0006
min_lr: 0.00006
micro_batch_size: 8
max_step: 16000
# xformers swiglu is not compatible with activation checkpointing
use_activation_checkpoint: False 
use_xformers_swiglu: True
lr_schedule: "cosine"
# Logging 
log_step_interval: 2
 

# Fixed Hyperparam
weight_decay: 0.1
beta1: 0.9
beta2: 0.95
grad_clip: 1.0
warmup_steps: 200
num_of_devices: 8

# Data
train_data_dir:  data/slim_star_combined
val_data_dir: data/slim_star_combined
train_data_config:
  - - "train_slim"
    - 0.693584
  # - - "train_star"
  #   - 0.306416
val_data_config:
  - - "validation"
    - 1.0
seed: 3407

# Resume
resume: false
pretrained_path: None